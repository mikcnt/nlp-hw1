{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150e13c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from pprint import pprint\n",
    "import jsonlines\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "\n",
    "\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343adcff",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cab6d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_pickle(data: dict, path: str) -> None:\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_pickle(path: str) -> dict:\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e69f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving / loading models\n",
    "class Checkpoint:\n",
    "    def __init__(self, path: str, resume=False):\n",
    "        self.path = path\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        self.resume = resume\n",
    "\n",
    "    def load(self, model, optimizer, id_path=\"\"):\n",
    "        if (not self.resume) and id_path == \"\":\n",
    "            raise RuntimeError()\n",
    "        if self.resume:\n",
    "            id_path = sorted(os.listdir(self.path))[-1]\n",
    "        self.checkpoint = torch.load(\n",
    "            os.path.join(self.path, id_path), map_location=lambda storage, loc: storage\n",
    "        )\n",
    "        if self.checkpoint == None:\n",
    "            raise RuntimeError(\"Checkpoint empty.\")\n",
    "        epoch = self.checkpoint[\"epoch\"]\n",
    "        model.load_state_dict(self.checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(self.checkpoint[\"optimizer_state_dict\"])\n",
    "        losses = self.checkpoint[\"losses\"]\n",
    "        accuracies = self.checkpoint[\"accuracies\"]\n",
    "        return (model, optimizer, epoch, losses, accuracies)\n",
    "\n",
    "    def save(self, model, optimizer, epoch, losses, accuracies):\n",
    "        model_checkpoint = {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"losses\": losses,\n",
    "            \"accuracies\": accuracies\n",
    "        }\n",
    "        checkpoint_name = \"{}.pth\".format(str(epoch).zfill(3))\n",
    "        complete_path = os.path.join(self.path, checkpoint_name)\n",
    "        torch.save(model_checkpoint, complete_path)\n",
    "        return\n",
    "\n",
    "    def load_just_model(self, model, id_path=\"\"):\n",
    "        if self.resume:\n",
    "            id_path = sorted(os.listdir(self.path))[-1]\n",
    "        self.checkpoint = torch.load(\n",
    "            os.path.join(self.path, id_path), map_location=lambda storage, loc: storage\n",
    "        )\n",
    "        if self.checkpoint == None:\n",
    "            raise RuntimeError(\"Checkpoint empty.\")\n",
    "        model.load_state_dict(self.checkpoint[\"model_state_dict\"])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b86528b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence: str) -> str:\n",
    "    # lowercase sentence\n",
    "    sentence = sentence.lower()\n",
    "    # remove punctuation\n",
    "    sentence = re.sub('[^\\w\\s]', ' ', sentence)\n",
    "    # replace multiple adjacent spaces with one single space\n",
    "    sentence = re.sub(' +', ' ', sentence).strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c3c5e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embeddings_dictionary(path: str) -> Dict[str, torch.Tensor]:\n",
    "    word_vectors = dict()\n",
    "    with open(path) as f:\n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "\n",
    "            word, *vector = line.strip().split(' ')\n",
    "            vector = torch.tensor([float(c) for c in vector])\n",
    "\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb83b37",
   "metadata": {},
   "source": [
    "# Create word embedding with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eded760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer(sentence: str, marker: str) -> List[str]:\n",
    "    tokens = sentence.split()\n",
    "    for i, tk in enumerate(tokens):\n",
    "        if marker in tk:\n",
    "            target_position = i\n",
    "            tokens[i] = tk[20:]\n",
    "    return tokens, target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c335989e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokensEmbedder(object):\n",
    "    def __init__(self, word_vectors: Dict[str, torch.Tensor]) -> None:\n",
    "        self.word_vectors = word_vectors\n",
    "    \n",
    "    def compute_embeddings(self, tokens: List[str]) -> List[torch.Tensor]:\n",
    "        word_embeddings = []\n",
    "        for w in tokens:\n",
    "            word_embeddings.append(self.word_vectors[w] if w in self.word_vectors else self.word_vectors['<unk>'])\n",
    "        \n",
    "        return word_embeddings\n",
    "    \n",
    "    def aggregate_embeddings(self, tokens: List[str], target_position: int) -> torch.Tensor:\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, tokens: List[str], target_position: int) -> torch.Tensor:\n",
    "        return self.aggregate_embeddings(tokens, target_position)\n",
    "\n",
    "class AverageEmbedder(TokensEmbedder):\n",
    "    def __init__(self, word_vectors: Dict[str, torch.Tensor]) -> None:\n",
    "        self.word_vectors = word_vectors\n",
    "    \n",
    "    def aggregate_embeddings(self, tokens: List[str], target_position: int) -> torch.Tensor:\n",
    "        embeddings = torch.stack(self.compute_embeddings(tokens))\n",
    "        return torch.mean(embeddings, dim=0)\n",
    "\n",
    "class WeightedAverageEmbedder(TokensEmbedder):\n",
    "    def __init__(self, word_vectors: Dict[str, torch.Tensor], max_weight: float = 1.0, min_weight: float = 0.0) -> None:\n",
    "        self.word_vectors = word_vectors\n",
    "        self.max_weight = max_weight\n",
    "        self.min_weight = min_weight\n",
    "    \n",
    "    def aggregate_embeddings(self, tokens: List[str], target_position: int) -> torch.Tensor:\n",
    "        embeddings = torch.stack(self.compute_embeddings(tokens))\n",
    "        # aliases for readibility\n",
    "        n = len(embeddings)\n",
    "        t = target_position\n",
    "\n",
    "        # weights from 1 to 0\n",
    "        weights = torch.linspace(self.max_weight, self.min_weight, n).unsqueeze(1)\n",
    "        # weights = torch.exp(reversed(torch.arange(n, dtype=torch.float32))).unsqueeze(1)\n",
    "\n",
    "        # weighted vector\n",
    "        new_vectors = embeddings\n",
    "\n",
    "        # weighted average\n",
    "\n",
    "        # right of the target word\n",
    "        new_vectors[t:] = new_vectors[t:] * weights[:n - t]\n",
    "        # left of the target word\n",
    "        new_vectors[:t] = new_vectors[:t] * reversed(weights[1:t + 1])\n",
    "\n",
    "        # denominator (sum of the weights)\n",
    "        weights_sum = weights[:n - t].sum() + weights[1:t + 1].sum()\n",
    "\n",
    "        return new_vectors.sum(dim=0) / weights_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d9b692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86946d605048426aaa24fdcab60775f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_path = 'embeddings/glove.6B.300d.txt'\n",
    "\n",
    "word_vectors = embeddings_dictionary(embedding_path)\n",
    "\n",
    "# <unk> token\n",
    "# https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt\n",
    "# unk_embedding = '0.22418134 -0.28881392 0.13854356 0.00365387 -0.12870757 0.10243822 0.061626635 0.07318011 -0.061350107 -1.3477012 0.42037755 -0.063593924 -0.09683349 0.18086134 0.23704372 0.014126852 0.170096 -1.1491593 0.31497982 0.06622181 0.024687296 0.076693475 0.13851812 0.021302193 -0.06640582 -0.010336159 0.13523154 -0.042144544 -0.11938788 0.006948221 0.13333307 -0.18276379 0.052385733 0.008943111 -0.23957317 0.08500333 -0.006894406 0.0015864656 0.063391194 0.19177166 -0.13113557 -0.11295479 -0.14276934 0.03413971 -0.034278486 -0.051366422 0.18891625 -0.16673574 -0.057783455 0.036823478 0.08078679 0.022949161 0.033298038 0.011784158 0.05643189 -0.042776518 0.011959623 0.011552498 -0.0007971594 0.11300405 -0.031369694 -0.0061559738 -0.009043574 -0.415336 -0.18870236 0.13708843 0.005911723 -0.113035575 -0.030096142 -0.23908928 -0.05354085 -0.044904727 -0.20228513 0.0065645403 -0.09578946 -0.07391877 -0.06487607 0.111740574 -0.048649278 -0.16565254 -0.052037314 -0.078968436 0.13684988 0.0757494 -0.006275573 0.28693774 0.52017444 -0.0877165 -0.33010918 -0.1359622 0.114895485 -0.09744406 0.06269521 0.12118575 -0.08026362 0.35256687 -0.060017522 -0.04889904 -0.06828978 0.088740796 0.003964443 -0.0766291 0.1263925 0.07809314 -0.023164088 -0.5680669 -0.037892066 -0.1350967 -0.11351585 -0.111434504 -0.0905027 0.25174105 -0.14841858 0.034635577 -0.07334565 0.06320108 -0.038343467 -0.05413284 0.042197507 -0.090380974 -0.070528865 -0.009174437 0.009069661 0.1405178 0.02958134 -0.036431845 -0.08625681 0.042951006 0.08230793 0.0903314 -0.12279937 -0.013899368 0.048119213 0.08678239 -0.14450377 -0.04424887 0.018319942 0.015026873 -0.100526 0.06021201 0.74059093 -0.0016333034 -0.24960588 -0.023739101 0.016396184 0.11928964 0.13950661 -0.031624354 -0.01645025 0.14079992 -0.0002824564 -0.08052984 -0.0021310581 -0.025350995 0.086938225 0.14308536 0.17146006 -0.13943303 0.048792403 0.09274929 -0.053167373 0.031103406 0.012354865 0.21057427 0.32618305 0.18015954 -0.15881181 0.15322933 -0.22558987 -0.04200665 0.0084689725 0.038156632 0.15188617 0.13274793 0.113756925 -0.095273495 -0.049490947 -0.10265804 -0.27064866 -0.034567792 -0.018810693 -0.0010360252 0.10340131 0.13883452 0.21131058 -0.01981019 0.1833468 -0.10751636 -0.03128868 0.02518242 0.23232952 0.042052146 0.11731903 -0.15506615 0.0063580726 -0.15429358 0.1511722 0.12745973 0.2576985 -0.25486213 -0.0709463 0.17983761 0.054027 -0.09884228 -0.24595179 -0.093028545 -0.028203879 0.094398156 0.09233813 0.029291354 0.13110267 0.15682974 -0.016919162 0.23927948 -0.1343307 -0.22422817 0.14634751 -0.064993896 0.4703685 -0.027190214 0.06224946 -0.091360025 0.21490277 -0.19562101 -0.10032754 -0.09056772 -0.06203493 -0.18876675 -0.10963594 -0.27734384 0.12616494 -0.02217992 -0.16058226 -0.080475815 0.026953284 0.110732645 0.014894041 0.09416802 0.14299914 -0.1594008 -0.066080004 -0.007995227 -0.11668856 -0.13081996 -0.09237365 0.14741232 0.09180138 0.081735 0.3211204 -0.0036552632 -0.047030564 -0.02311798 0.048961394 0.08669574 -0.06766279 -0.50028914 -0.048515294 0.14144728 -0.032994404 -0.11954345 -0.14929578 -0.2388355 -0.019883996 -0.15917352 -0.052084364 0.2801028 -0.0029121689 -0.054581646 -0.47385484 0.17112483 -0.12066923 -0.042173345 0.1395337 0.26115036 0.012869649 0.009291686 -0.0026459037 -0.075331464 0.017840583 -0.26869613 -0.21820338 -0.17084768 -0.1022808 -0.055290595 0.13513643 0.12362477 -0.10980586 0.13980341 -0.20233242 0.08813751 0.3849736 -0.10653763 -0.06199595 0.028849555 0.03230154 0.023856193 0.069950655 0.19310954 -0.077677034 -0.144811'\n",
    "# unk_embedding = unk_embedding.strip().split()\n",
    "# word_vectors['<unk>'] = torch.tensor([float(c) for c in unk_embedding])\n",
    "# word_vectors['<unk>'] = torch.rand(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16f2da44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_pickle(word_vectors, 'embeddings/vocabulary_tensors.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08aaac3",
   "metadata": {},
   "source": [
    "# Dataset class using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50b29d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WiCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path: str, marker: str, embedder: TokensEmbedder):\n",
    "        self.data = []\n",
    "        self.marker = marker\n",
    "        self.embedder = embedder\n",
    "\n",
    "        self.create_dataset(dataset_path)\n",
    "        \n",
    "    \n",
    "    def create_dataset(self, dataset_path: str) -> None:\n",
    "        with jsonlines.open(dataset_path, 'r') as f:\n",
    "            for i, line in enumerate(f.iter()):\n",
    "                # load sentences\n",
    "                start1 = int(line['start1'])\n",
    "                start2 = int(line['start2'])\n",
    "                s1 = line['sentence1']\n",
    "                s2 = line['sentence2']\n",
    "                # insert special characters to locate target word after preprocessing\n",
    "                s1 = s1[:start1] + self.marker + s1[start1:]\n",
    "                s2 = s2[:start2] + self.marker + s2[start2:]\n",
    "                \n",
    "                # preprocessing\n",
    "                s1 = preprocess(s1)\n",
    "                s2 = preprocess(s2)\n",
    "                \n",
    "                # tokenization\n",
    "                t1, target_position1 = custom_tokenizer(s1, self.marker)\n",
    "                t2, target_position2 = custom_tokenizer(s2, self.marker)\n",
    "                \n",
    "                # convert tokens to embeddings and aggregate\n",
    "                v1 = self.embedder(t1, target_position1)\n",
    "                v2 = self.embedder(t2, target_position2)\n",
    "                \n",
    "                # concatenate vectors\n",
    "                sentence_vector = torch.cat((v1, v2))\n",
    "                \n",
    "                label = torch.tensor(1, dtype=torch.float32) if line['label'] == 'True' else torch.tensor(0, dtype=torch.float32)\n",
    "                self.data.append((sentence_vector, label))\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3f395",
   "metadata": {},
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9fad87f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        num_layers: int,\n",
    "        hidden_dim: int,\n",
    "        activation: Callable[[torch.Tensor], torch.Tensor],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.first_layer = nn.Linear(in_features=n_features, out_features=hidden_dim)\n",
    "\n",
    "        self.layers = (\n",
    "            nn.ModuleList()\n",
    "        )\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(\n",
    "                nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "            )\n",
    "            \n",
    "        self.activation = activation\n",
    "        \n",
    "        self.last_layer = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, meshgrid: torch.Tensor) -> torch.Tensor:\n",
    "        out = meshgrid\n",
    "\n",
    "        out = self.first_layer(\n",
    "            out\n",
    "        )  # First linear layer, transforms the hidden dimensions from `n_features` (embedding dimension) to `hidden_dim`\n",
    "        for layer in self.layers:  # Apply `k` (linear, activation) layer\n",
    "            out = layer(out)\n",
    "            out = self.activation(out)\n",
    "            out = self.dropout(out)\n",
    "        out = self.last_layer(\n",
    "            out\n",
    "        )  # Last linear layer to bring the `hiddem_dim` features to a binary space (`True`/`False`)\n",
    "        \n",
    "        out = self.sigmoid(out)\n",
    "        return out.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244db1e5",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89549139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(epochs: int,\n",
    "        model: nn.Module,\n",
    "        criterion: nn.Module,\n",
    "        opt: torch.optim.Optimizer,\n",
    "        train_dl: torch.utils.data.DataLoader,\n",
    "        valid_dl: torch.utils.data.DataLoader,\n",
    "        checkpoint: Checkpoint = None\n",
    "       ) -> None:\n",
    "\n",
    "    losses = {'train': [], 'val': []}\n",
    "    accuracies = {'train': [], 'val': []}\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        losses_train = 0\n",
    "        d_train, n_train = 0, 0\n",
    "\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            \n",
    "            pred = model(xb)\n",
    "            \n",
    "            loss_train = criterion(pred, yb)\n",
    "            loss_train.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            losses_train += loss_train.item()\n",
    "            \n",
    "            pred = torch.round(pred)\n",
    "            # number of predictions\n",
    "            d_train += pred.shape[0]\n",
    "            # number of correct predictions\n",
    "            n_train += (yb == pred).int().sum()\n",
    "            \n",
    "            \n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses_val = 0\n",
    "            d_val, n_val = 0, 0\n",
    "            for xb, yb in valid_dl:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                \n",
    "                pred = model(xb)\n",
    "\n",
    "                loss_val = criterion(pred, yb)\n",
    "                losses_val += loss_val.item()\n",
    "                \n",
    "\n",
    "                pred = torch.round(pred)\n",
    "                # number of predictions\n",
    "                d_val += pred.shape[0]\n",
    "                # number of correct predictions\n",
    "                n_val += (yb == pred).int().sum().item()\n",
    "                \n",
    "        loss_train = losses_train / d_train\n",
    "        loss_val = losses_val / d_val\n",
    "        \n",
    "        acc_train = n_train / d_train\n",
    "        acc_val = n_val / d_val\n",
    "        \n",
    "        losses['train'].append(loss_train)\n",
    "        losses['val'].append(loss_val)\n",
    "        \n",
    "        accuracies['train'].append(acc_train)\n",
    "        accuracies['val'].append(acc_val)\n",
    "\n",
    "        if checkpoint:\n",
    "            checkpoint.save(model, opt, epoch, losses, accuracies)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch} \\t T. Loss = {loss_train:.4f}, V. Loss = {loss_val:.4f}, T. Accuracy {acc_train:.2f}, V. Accuracy {acc_val:.2f}.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b843c3b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d948f91d77a1474590581418d16b60e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t T. Loss = 0.0055, V. Loss = 0.0055, T. Accuracy 0.50, V. Accuracy 0.50.\n",
      "Epoch 1 \t T. Loss = 0.0055, V. Loss = 0.0055, T. Accuracy 0.50, V. Accuracy 0.50.\n",
      "Epoch 2 \t T. Loss = 0.0055, V. Loss = 0.0055, T. Accuracy 0.51, V. Accuracy 0.52.\n",
      "Epoch 3 \t T. Loss = 0.0055, V. Loss = 0.0055, T. Accuracy 0.50, V. Accuracy 0.57.\n",
      "Epoch 4 \t T. Loss = 0.0055, V. Loss = 0.0055, T. Accuracy 0.50, V. Accuracy 0.53.\n",
      "Epoch 5 \t T. Loss = 0.0055, V. Loss = 0.0055, T. Accuracy 0.51, V. Accuracy 0.58.\n",
      "Epoch 6 \t T. Loss = 0.0054, V. Loss = 0.0055, T. Accuracy 0.53, V. Accuracy 0.57.\n",
      "Epoch 7 \t T. Loss = 0.0054, V. Loss = 0.0055, T. Accuracy 0.56, V. Accuracy 0.57.\n",
      "Epoch 8 \t T. Loss = 0.0054, V. Loss = 0.0054, T. Accuracy 0.56, V. Accuracy 0.57.\n",
      "Epoch 9 \t T. Loss = 0.0053, V. Loss = 0.0054, T. Accuracy 0.58, V. Accuracy 0.57.\n",
      "Epoch 10 \t T. Loss = 0.0053, V. Loss = 0.0054, T. Accuracy 0.59, V. Accuracy 0.57.\n",
      "Epoch 11 \t T. Loss = 0.0052, V. Loss = 0.0054, T. Accuracy 0.61, V. Accuracy 0.59.\n",
      "Epoch 12 \t T. Loss = 0.0052, V. Loss = 0.0054, T. Accuracy 0.62, V. Accuracy 0.58.\n",
      "Epoch 13 \t T. Loss = 0.0051, V. Loss = 0.0053, T. Accuracy 0.63, V. Accuracy 0.59.\n",
      "Epoch 14 \t T. Loss = 0.0050, V. Loss = 0.0053, T. Accuracy 0.64, V. Accuracy 0.60.\n",
      "Epoch 15 \t T. Loss = 0.0050, V. Loss = 0.0053, T. Accuracy 0.65, V. Accuracy 0.61.\n",
      "Epoch 16 \t T. Loss = 0.0050, V. Loss = 0.0053, T. Accuracy 0.66, V. Accuracy 0.61.\n",
      "Epoch 17 \t T. Loss = 0.0049, V. Loss = 0.0052, T. Accuracy 0.66, V. Accuracy 0.62.\n",
      "Epoch 18 \t T. Loss = 0.0048, V. Loss = 0.0052, T. Accuracy 0.67, V. Accuracy 0.61.\n",
      "Epoch 19 \t T. Loss = 0.0048, V. Loss = 0.0052, T. Accuracy 0.68, V. Accuracy 0.62.\n",
      "Epoch 20 \t T. Loss = 0.0047, V. Loss = 0.0052, T. Accuracy 0.69, V. Accuracy 0.62.\n",
      "Epoch 21 \t T. Loss = 0.0046, V. Loss = 0.0052, T. Accuracy 0.70, V. Accuracy 0.63.\n",
      "Epoch 22 \t T. Loss = 0.0045, V. Loss = 0.0051, T. Accuracy 0.71, V. Accuracy 0.65.\n",
      "Epoch 23 \t T. Loss = 0.0044, V. Loss = 0.0052, T. Accuracy 0.72, V. Accuracy 0.65.\n",
      "Epoch 24 \t T. Loss = 0.0044, V. Loss = 0.0052, T. Accuracy 0.73, V. Accuracy 0.64.\n",
      "Epoch 25 \t T. Loss = 0.0043, V. Loss = 0.0052, T. Accuracy 0.73, V. Accuracy 0.63.\n",
      "Epoch 26 \t T. Loss = 0.0042, V. Loss = 0.0052, T. Accuracy 0.74, V. Accuracy 0.65.\n",
      "Epoch 27 \t T. Loss = 0.0042, V. Loss = 0.0054, T. Accuracy 0.74, V. Accuracy 0.64.\n",
      "Epoch 28 \t T. Loss = 0.0041, V. Loss = 0.0053, T. Accuracy 0.76, V. Accuracy 0.66.\n",
      "Epoch 29 \t T. Loss = 0.0040, V. Loss = 0.0054, T. Accuracy 0.76, V. Accuracy 0.65.\n",
      "Epoch 30 \t T. Loss = 0.0040, V. Loss = 0.0054, T. Accuracy 0.76, V. Accuracy 0.65.\n",
      "Epoch 31 \t T. Loss = 0.0039, V. Loss = 0.0055, T. Accuracy 0.77, V. Accuracy 0.64.\n",
      "Epoch 32 \t T. Loss = 0.0038, V. Loss = 0.0055, T. Accuracy 0.78, V. Accuracy 0.65.\n",
      "Epoch 33 \t T. Loss = 0.0037, V. Loss = 0.0058, T. Accuracy 0.79, V. Accuracy 0.65.\n",
      "Epoch 34 \t T. Loss = 0.0036, V. Loss = 0.0056, T. Accuracy 0.79, V. Accuracy 0.64.\n",
      "Epoch 35 \t T. Loss = 0.0036, V. Loss = 0.0057, T. Accuracy 0.80, V. Accuracy 0.65.\n",
      "Epoch 36 \t T. Loss = 0.0035, V. Loss = 0.0058, T. Accuracy 0.81, V. Accuracy 0.65.\n",
      "Epoch 37 \t T. Loss = 0.0034, V. Loss = 0.0059, T. Accuracy 0.81, V. Accuracy 0.63.\n",
      "Epoch 38 \t T. Loss = 0.0034, V. Loss = 0.0059, T. Accuracy 0.81, V. Accuracy 0.64.\n",
      "Epoch 39 \t T. Loss = 0.0033, V. Loss = 0.0059, T. Accuracy 0.82, V. Accuracy 0.64.\n",
      "Epoch 40 \t T. Loss = 0.0032, V. Loss = 0.0061, T. Accuracy 0.82, V. Accuracy 0.64.\n",
      "Epoch 41 \t T. Loss = 0.0031, V. Loss = 0.0062, T. Accuracy 0.83, V. Accuracy 0.64.\n",
      "Epoch 42 \t T. Loss = 0.0030, V. Loss = 0.0064, T. Accuracy 0.84, V. Accuracy 0.64.\n",
      "Epoch 43 \t T. Loss = 0.0030, V. Loss = 0.0064, T. Accuracy 0.84, V. Accuracy 0.64.\n",
      "Epoch 44 \t T. Loss = 0.0029, V. Loss = 0.0066, T. Accuracy 0.86, V. Accuracy 0.63.\n",
      "Epoch 45 \t T. Loss = 0.0029, V. Loss = 0.0066, T. Accuracy 0.85, V. Accuracy 0.63.\n",
      "Epoch 46 \t T. Loss = 0.0028, V. Loss = 0.0067, T. Accuracy 0.86, V. Accuracy 0.64.\n",
      "Epoch 47 \t T. Loss = 0.0027, V. Loss = 0.0069, T. Accuracy 0.86, V. Accuracy 0.62.\n",
      "Epoch 48 \t T. Loss = 0.0027, V. Loss = 0.0067, T. Accuracy 0.86, V. Accuracy 0.64.\n",
      "Epoch 49 \t T. Loss = 0.0026, V. Loss = 0.0070, T. Accuracy 0.87, V. Accuracy 0.63.\n"
     ]
    }
   ],
   "source": [
    "train_path = 'data/train.jsonl'\n",
    "dev_path = 'data/dev.jsonl'\n",
    "\n",
    "random.seed(42)\n",
    "marker = ''.join(random.choices(string.ascii_lowercase, k=20))\n",
    "\n",
    "embedder = WeightedAverageEmbedder(word_vectors, 1, 0)\n",
    "\n",
    "train_dataset = WiCDataset(train_path, marker, embedder)\n",
    "val_dataset = WiCDataset(dev_path, marker, embedder)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = nn.BCELoss()\n",
    "model = MLP(n_features=600,\n",
    "            num_layers=5, \n",
    "            hidden_dim=150, \n",
    "            activation=torch.nn.functional.relu).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)\n",
    "\n",
    "checkpoint = Checkpoint(path='checkpoints')\n",
    "\n",
    "fit(50, model, criterion, optimizer, train_loader, val_loader, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dda9b2-f257-429c-9d8a-39418ddeda39",
   "metadata": {},
   "source": [
    "# Reccurent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f4a58ef-bf08-4ed0-b292-52acd6ae93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_index = dict()\n",
    "vectors_store = []\n",
    "\n",
    "# pad token, index = 0\n",
    "vectors_store.append(torch.rand(300))\n",
    "\n",
    "# unk token, index = 1\n",
    "vectors_store.append(torch.rand(300))\n",
    "\n",
    "# save index for each word\n",
    "for word, vector in word_vectors.items():\n",
    "    word_index[word] = len(vectors_store)\n",
    "    vectors_store.append(vector)\n",
    "\n",
    "word_index = defaultdict(lambda: 1, word_index)  # default dict returns 1 (unk token) when unknown word\n",
    "vectors_store = torch.stack(vectors_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d589283-ccb4-4ca4-bf8a-21ef709b79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review2indices(review: str) -> torch.Tensor:\n",
    "    return torch.tensor([word_index[word] for word in review.split(' ')], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c0903a5-f013-4a60-ba7d-5307a3a98c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_collate_fn(\n",
    "    data_elements: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] # list of (x, y) pairs\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    X1 = [el[0] for el in data_elements]  # list of index tensors\n",
    "    X2 = [el[1] for el in data_elements]  # list of index tensors\n",
    "    \n",
    "\n",
    "    # sizes of the sentences\n",
    "    # to implement the many-to-one strategy\n",
    "    X1_lengths = torch.tensor([x.size(0) for x in X1], dtype=torch.long)\n",
    "    X2_lengths = torch.tensor([x.size(0) for x in X2], dtype=torch.long)\n",
    "    \n",
    "\n",
    "    X1 = torch.nn.utils.rnn.pad_sequence(X1, batch_first=True, padding_value=0)  #  shape (batch_size x max_seq_len)\n",
    "    X2 = torch.nn.utils.rnn.pad_sequence(X2, batch_first=True, padding_value=0)  #  shape (batch_size x max_seq_len)\n",
    "    \n",
    "\n",
    "    y = [el[2] for el in data_elements]\n",
    "    y = torch.tensor(y)\n",
    "\n",
    "    return X1, X1_lengths, X2, X2_lengths, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2014a25e-c034-4b1d-9673-03f440fe60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings1 = ['the cat is on the table', 'hello old friend', 'hi my name is michele what is yours', 'brother']\n",
    "labels1 = [torch.tensor(0) for _ in range(len(strings))]\n",
    "\n",
    "X1, X1_length, y1 = rnn_collate_fn(list(zip([review2indices(string) for string in strings1], labels1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "13e14a6b-0178-4f9b-bc8c-e5ec837b4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings2 = ['my father is happy', 'hello man ', 'sister and up', \"cause because chicken\"]\n",
    "labels2 = [torch.tensor(0) for _ in range(len(strings))]\n",
    "\n",
    "X2, X2_length, y2 = rnn_collate_fn(list(zip([review2indices(string) for string in strings2], labels2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "228d28fe-2aeb-45cc-a8ae-e3481bc1bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vectors_store: torch.Tensor,\n",
    "        n_hidden: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(vectors_store)\n",
    "\n",
    "        # recurrent layer\n",
    "        self.rnn = torch.nn.LSTM(input_size=vectors_store.size(1), hidden_size=n_hidden, num_layers=1, batch_first=True)\n",
    "\n",
    "        # classification head\n",
    "        self.lin1 = torch.nn.Linear(2 * n_hidden, 2 * n_hidden)\n",
    "        self.lin2 = torch.nn.Linear(2 * n_hidden, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def _compute_embedding(self, X, X_length):\n",
    "        # embedding words from indices\n",
    "        embedding_out = self.embedding(X)\n",
    "\n",
    "        # recurrent encoding\n",
    "        recurrent_out = self.rnn(embedding_out)[0]\n",
    "        # here we utilize the sequences length to retrieve the last token \n",
    "        # output for each sequence\n",
    "        batch_size, seq_len, hidden_size = recurrent_out.shape\n",
    "\n",
    "        # we flatten the recurrent output\n",
    "        # now I have a long sequence of batch x seq_len vectors \n",
    "        flattened_out = recurrent_out.reshape(-1, hidden_size)\n",
    "        \n",
    "        # and we use a simple trick to compute a tensor of the indices \n",
    "        # of the last token in each batch element\n",
    "        last_word_relative_indices = X_length - 1\n",
    "        # tensor of the start offsets of each element in the batch\n",
    "        sequences_offsets = torch.arange(batch_size, device='cuda:0') * seq_len\n",
    "        # e.g. (0, 5, 10, 15, ) + ( 3, 2, 1, 4 ) = ( 3, 7, 11, 19 )\n",
    "        summary_vectors_indices = sequences_offsets + last_word_relative_indices\n",
    "\n",
    "        # finally we retrieve the vectors that should summarize every review.\n",
    "        # (i.e. the last token in the sequence)\n",
    "        summary_vectors = flattened_out[summary_vectors_indices]\n",
    "        return summary_vectors\n",
    "    \n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        X1: torch.Tensor,\n",
    "        X1_length: torch.Tensor,\n",
    "        X2: torch.Tensor,\n",
    "        X2_length: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        summary_vectors_1 = self._compute_embedding(X1, X1_length)\n",
    "        summary_vectors_2 = self._compute_embedding(X2, X2_length)\n",
    "        \n",
    "        summary_vectors = torch.cat((summary_vectors_1, summary_vectors_2), dim=1)\n",
    "\n",
    "        # now we can classify the reviews with a feedforward pass on the summary\n",
    "        # vectors\n",
    "        out = self.lin1(summary_vectors)\n",
    "        out = torch.relu(out)\n",
    "        out = self.lin2(out).squeeze(1)\n",
    "\n",
    "        # compute logits (which are simply the out variable)\n",
    "        # and the actual probability distribution (pred, as it is the predicted distribution)\n",
    "        logits = out\n",
    "        pred = self.sigmoid(logits)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7299a486-47fc-40e1-8ca7-de9663b05557",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path: str):\n",
    "        self.data = []\n",
    "\n",
    "        self.create_dataset(dataset_path)\n",
    "        \n",
    "    \n",
    "    def create_dataset(self, dataset_path: str) -> None:\n",
    "        with jsonlines.open(dataset_path, 'r') as f:\n",
    "            for i, line in enumerate(f.iter()):\n",
    "                # load sentences\n",
    "                start1 = int(line['start1'])\n",
    "                start2 = int(line['start2'])\n",
    "                s1 = line['sentence1']\n",
    "                s2 = line['sentence2']\n",
    "\n",
    "                # preprocessing\n",
    "                s1 = preprocess(s1)\n",
    "                s2 = preprocess(s2)\n",
    "                \n",
    "                # sentences to indices\n",
    "                i1 = review2indices(s1)\n",
    "                i2 = review2indices(s2)\n",
    "\n",
    "                label = torch.tensor(1, dtype=torch.float32) if line['label'] == 'True' else torch.tensor(0, dtype=torch.float32)\n",
    "                self.data.append((i1, i2, label))\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3e893ff-a395-4576-8be2-5dd492597bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_rnn(epochs: int,\n",
    "            model: nn.Module,\n",
    "            criterion: nn.Module,\n",
    "            opt: torch.optim.Optimizer,\n",
    "            train_dl: torch.utils.data.DataLoader,\n",
    "            valid_dl: torch.utils.data.DataLoader,\n",
    "            checkpoint: Checkpoint = None\n",
    "           ) -> None:\n",
    "\n",
    "    losses = {'train': [], 'val': []}\n",
    "    accuracies = {'train': [], 'val': []}\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        losses_train = 0\n",
    "        d_train, n_train = 0, 0\n",
    "\n",
    "        model.train()\n",
    "        for x1, x1_length, x2, x2_length, yb in train_dl:\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x1_length = x1_length.to(device)\n",
    "            x2_length = x2_length.to(device)\n",
    "            \n",
    "            yb = yb.to(device)\n",
    "\n",
    "            pred = model(x1, x1_length, x2, x2_length)\n",
    "            \n",
    "            loss_train = criterion(pred, yb)\n",
    "            loss_train.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            losses_train += loss_train.item()\n",
    "            \n",
    "            pred = torch.round(pred)\n",
    "            # number of predictions\n",
    "            d_train += pred.shape[0]\n",
    "            # number of correct predictions\n",
    "            n_train += (yb == pred).int().sum()\n",
    "            \n",
    "            \n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses_val = 0\n",
    "            d_val, n_val = 0, 0\n",
    "            for x1, x1_length, x2, x2_length, yb in train_dl:\n",
    "                x1 = x1.to(device)\n",
    "                x2 = x2.to(device)\n",
    "                x1_length = x1_length.to(device)\n",
    "                x2_length = x2_length.to(device)\n",
    "                yb = yb.to(device)\n",
    "                \n",
    "                pred = model(x1, x1_length, x2, x2_length)\n",
    "\n",
    "                loss_val = criterion(pred, yb)\n",
    "                losses_val += loss_val.item()\n",
    "                \n",
    "\n",
    "                pred = torch.round(pred)\n",
    "                # number of predictions\n",
    "                d_val += pred.shape[0]\n",
    "                # number of correct predictions\n",
    "                n_val += (yb == pred).int().sum().item()\n",
    "                \n",
    "        loss_train = losses_train / d_train\n",
    "        loss_val = losses_val / d_val\n",
    "        \n",
    "        acc_train = n_train / d_train\n",
    "        acc_val = n_val / d_val\n",
    "        \n",
    "        losses['train'].append(loss_train)\n",
    "        losses['val'].append(loss_val)\n",
    "        \n",
    "        accuracies['train'].append(acc_train)\n",
    "        accuracies['val'].append(acc_val)\n",
    "\n",
    "        if checkpoint:\n",
    "            checkpoint.save(model, opt, epoch, losses, accuracies)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch} \\t T. Loss = {loss_train:.4f}, V. Loss = {loss_val:.4f}, T. Accuracy {acc_train:.2f}, V. Accuracy {acc_val:.2f}.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7505040d-860a-4cb0-818d-111b43e5e549",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e128c37f6246a09aae80c595797944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t T. Loss = 0.0055, V. Loss = 0.0055, T. Accuracy 0.50, V. Accuracy 0.53.\n",
      "Epoch 1 \t T. Loss = 0.0055, V. Loss = 0.0054, T. Accuracy 0.53, V. Accuracy 0.56.\n",
      "Epoch 2 \t T. Loss = 0.0054, V. Loss = 0.0054, T. Accuracy 0.55, V. Accuracy 0.57.\n",
      "Epoch 3 \t T. Loss = 0.0054, V. Loss = 0.0054, T. Accuracy 0.56, V. Accuracy 0.57.\n",
      "Epoch 4 \t T. Loss = 0.0054, V. Loss = 0.0054, T. Accuracy 0.57, V. Accuracy 0.59.\n",
      "Epoch 5 \t T. Loss = 0.0054, V. Loss = 0.0054, T. Accuracy 0.58, V. Accuracy 0.59.\n",
      "Epoch 6 \t T. Loss = 0.0054, V. Loss = 0.0053, T. Accuracy 0.58, V. Accuracy 0.60.\n",
      "Epoch 7 \t T. Loss = 0.0053, V. Loss = 0.0053, T. Accuracy 0.59, V. Accuracy 0.59.\n",
      "Epoch 8 \t T. Loss = 0.0053, V. Loss = 0.0053, T. Accuracy 0.60, V. Accuracy 0.61.\n",
      "Epoch 9 \t T. Loss = 0.0052, V. Loss = 0.0052, T. Accuracy 0.60, V. Accuracy 0.60.\n",
      "Epoch 10 \t T. Loss = 0.0052, V. Loss = 0.0051, T. Accuracy 0.61, V. Accuracy 0.63.\n",
      "Epoch 11 \t T. Loss = 0.0051, V. Loss = 0.0050, T. Accuracy 0.63, V. Accuracy 0.64.\n",
      "Epoch 12 \t T. Loss = 0.0050, V. Loss = 0.0050, T. Accuracy 0.64, V. Accuracy 0.66.\n",
      "Epoch 13 \t T. Loss = 0.0050, V. Loss = 0.0048, T. Accuracy 0.65, V. Accuracy 0.67.\n",
      "Epoch 14 \t T. Loss = 0.0049, V. Loss = 0.0048, T. Accuracy 0.67, V. Accuracy 0.68.\n",
      "Epoch 15 \t T. Loss = 0.0048, V. Loss = 0.0047, T. Accuracy 0.68, V. Accuracy 0.69.\n",
      "Epoch 16 \t T. Loss = 0.0047, V. Loss = 0.0045, T. Accuracy 0.69, V. Accuracy 0.70.\n",
      "Epoch 17 \t T. Loss = 0.0046, V. Loss = 0.0044, T. Accuracy 0.70, V. Accuracy 0.72.\n",
      "Epoch 18 \t T. Loss = 0.0045, V. Loss = 0.0043, T. Accuracy 0.71, V. Accuracy 0.73.\n",
      "Epoch 19 \t T. Loss = 0.0044, V. Loss = 0.0042, T. Accuracy 0.72, V. Accuracy 0.74.\n",
      "Epoch 20 \t T. Loss = 0.0043, V. Loss = 0.0041, T. Accuracy 0.73, V. Accuracy 0.75.\n",
      "Epoch 21 \t T. Loss = 0.0041, V. Loss = 0.0040, T. Accuracy 0.75, V. Accuracy 0.77.\n",
      "Epoch 22 \t T. Loss = 0.0040, V. Loss = 0.0038, T. Accuracy 0.76, V. Accuracy 0.78.\n",
      "Epoch 23 \t T. Loss = 0.0039, V. Loss = 0.0037, T. Accuracy 0.77, V. Accuracy 0.79.\n",
      "Epoch 24 \t T. Loss = 0.0038, V. Loss = 0.0036, T. Accuracy 0.78, V. Accuracy 0.79.\n",
      "Epoch 25 \t T. Loss = 0.0036, V. Loss = 0.0034, T. Accuracy 0.79, V. Accuracy 0.81.\n",
      "Epoch 26 \t T. Loss = 0.0035, V. Loss = 0.0034, T. Accuracy 0.80, V. Accuracy 0.81.\n",
      "Epoch 27 \t T. Loss = 0.0034, V. Loss = 0.0032, T. Accuracy 0.81, V. Accuracy 0.83.\n",
      "Epoch 28 \t T. Loss = 0.0032, V. Loss = 0.0030, T. Accuracy 0.82, V. Accuracy 0.84.\n",
      "Epoch 29 \t T. Loss = 0.0031, V. Loss = 0.0029, T. Accuracy 0.83, V. Accuracy 0.85.\n",
      "Epoch 30 \t T. Loss = 0.0030, V. Loss = 0.0028, T. Accuracy 0.84, V. Accuracy 0.87.\n",
      "Epoch 31 \t T. Loss = 0.0028, V. Loss = 0.0026, T. Accuracy 0.85, V. Accuracy 0.87.\n",
      "Epoch 32 \t T. Loss = 0.0028, V. Loss = 0.0025, T. Accuracy 0.85, V. Accuracy 0.88.\n",
      "Epoch 33 \t T. Loss = 0.0026, V. Loss = 0.0023, T. Accuracy 0.87, V. Accuracy 0.89.\n",
      "Epoch 34 \t T. Loss = 0.0025, V. Loss = 0.0023, T. Accuracy 0.88, V. Accuracy 0.89.\n",
      "Epoch 35 \t T. Loss = 0.0024, V. Loss = 0.0021, T. Accuracy 0.88, V. Accuracy 0.90.\n",
      "Epoch 36 \t T. Loss = 0.0022, V. Loss = 0.0020, T. Accuracy 0.89, V. Accuracy 0.91.\n",
      "Epoch 37 \t T. Loss = 0.0021, V. Loss = 0.0020, T. Accuracy 0.90, V. Accuracy 0.91.\n",
      "Epoch 38 \t T. Loss = 0.0020, V. Loss = 0.0018, T. Accuracy 0.90, V. Accuracy 0.92.\n",
      "Epoch 39 \t T. Loss = 0.0019, V. Loss = 0.0017, T. Accuracy 0.91, V. Accuracy 0.93.\n",
      "Epoch 40 \t T. Loss = 0.0018, V. Loss = 0.0016, T. Accuracy 0.92, V. Accuracy 0.94.\n",
      "Epoch 41 \t T. Loss = 0.0016, V. Loss = 0.0015, T. Accuracy 0.93, V. Accuracy 0.94.\n",
      "Epoch 42 \t T. Loss = 0.0016, V. Loss = 0.0015, T. Accuracy 0.94, V. Accuracy 0.94.\n",
      "Epoch 43 \t T. Loss = 0.0015, V. Loss = 0.0013, T. Accuracy 0.94, V. Accuracy 0.95.\n",
      "Epoch 44 \t T. Loss = 0.0014, V. Loss = 0.0012, T. Accuracy 0.95, V. Accuracy 0.96.\n",
      "Epoch 45 \t T. Loss = 0.0013, V. Loss = 0.0011, T. Accuracy 0.95, V. Accuracy 0.97.\n",
      "Epoch 46 \t T. Loss = 0.0012, V. Loss = 0.0010, T. Accuracy 0.96, V. Accuracy 0.97.\n",
      "Epoch 47 \t T. Loss = 0.0011, V. Loss = 0.0009, T. Accuracy 0.96, V. Accuracy 0.98.\n",
      "Epoch 48 \t T. Loss = 0.0010, V. Loss = 0.0009, T. Accuracy 0.97, V. Accuracy 0.98.\n",
      "Epoch 49 \t T. Loss = 0.0009, V. Loss = 0.0008, T. Accuracy 0.97, V. Accuracy 0.98.\n"
     ]
    }
   ],
   "source": [
    "train_path = 'data/train.jsonl'\n",
    "dev_path = 'data/dev.jsonl'\n",
    "\n",
    "train_dataset = EmbeddingDataset(train_path)\n",
    "val_dataset = EmbeddingDataset(dev_path)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=rnn_collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=rnn_collate_fn)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "model = LSTMClassifier(vectors_store, 100).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)\n",
    "\n",
    "checkpoint = Checkpoint(path='checkpoints/rnn')\n",
    "\n",
    "fit_rnn(50, model, criterion, optimizer, train_loader, val_loader, checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
